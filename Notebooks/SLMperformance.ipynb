{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils.models import DistilBertBaseUncased\n",
    "from utils.evaluate import evaluate_slm_performance\n",
    "from utils.nlp import TextDatasetSLM\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 66.97M\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertBaseUncased()\n",
    "model.load_state_dict(torch.load(\"../Models/distilbert.pth\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.load(\"../Data/Testing/x_test.npy\", allow_pickle=True)\n",
    "ytest = np.load(\"../Data/Testing/y_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.41      0.56        22\n",
      "           1       0.60      0.51      0.55       251\n",
      "           2       0.76      0.62      0.68       163\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.59      0.50      0.54       158\n",
      "           5       0.70      0.83      0.76       693\n",
      "           6       0.75      0.64      0.69       148\n",
      "           7       0.65      0.62      0.63        52\n",
      "           8       0.67      0.39      0.49        46\n",
      "           9       0.55      0.55      0.55        77\n",
      "          10       0.62      0.64      0.63       352\n",
      "          11       0.72      0.65      0.68       502\n",
      "          12       0.59      0.56      0.57       212\n",
      "          13       0.42      0.32      0.36       125\n",
      "          14       0.66      0.66      0.66       304\n",
      "          15       0.79      0.58      0.67       145\n",
      "          16       0.70      0.58      0.64       168\n",
      "          17       0.59      0.52      0.56       223\n",
      "\n",
      "   micro avg       0.66      0.62      0.64      3663\n",
      "   macro avg       0.63      0.53      0.57      3663\n",
      "weighted avg       0.66      0.62      0.64      3663\n",
      " samples avg       0.69      0.66      0.64      3663\n",
      "\n",
      "Accuracy: 16.36%\n",
      "Hamming Loss: 0.0991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prash\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Prash\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "device= 'cuda'\n",
    "test_set = TextDatasetSLM(xtest, ytest, tokenizer)\n",
    "loader = DataLoader(test_set, 32, True)\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "model.to(device)\n",
    "with torch.inference_mode():\n",
    "    for batch in loader:\n",
    "        tokens = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        masks = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = batch['targets'].to(device, dtype=torch.float)\n",
    "        logits = model(tokens, masks)\n",
    "        preds = torch.round(torch.sigmoid(logits))\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "pred_labels = np.array(pred_labels)\n",
    "\n",
    "h_loss = hamming_loss(true_labels, pred_labels)\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Hamming Loss: {h_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
